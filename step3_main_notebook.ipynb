{"cells":[{"cell_type":"code","source":["# Configuration: DBR 7.0 ML and spark 3.0.0\n# ============================="],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Installing Libraries\n# ====================\n\n!pip install textblob"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Importing Necessary Libraries\n# =============================\n\nimport pyspark\nfrom pyspark.sql.functions import col\nfrom pyspark.sql import functions as F, SparkSession\nfrom pyspark.sql.types import FloatType"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Starting a Spark Session\n# ========================\n\nspark = SparkSession.builder.getOrCreate()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Loading DataFrame from Pre-saved Parquet File\n# =============================================\n\n# Importing libraries\nfrom pyspark.sql.types import *\n\n# Defining the schema\nschema = StructType([\\\n                      StructField(\"author\",StringType(),True),\\\n                      StructField(\"author_cakeday\",BooleanType(),False),\\\n                      StructField(\"created_utc_day\",IntegerType(),True),\\\n                      StructField(\"created_utc_hr\",IntegerType(),True),\\\n                      StructField(\"brand_safe\",BooleanType(),True),\\\n                      StructField(\"can_gild\",BooleanType(),True),\\\n                      StructField(\"domain\",StringType(),True),\\\n                      StructField(\"is_crosspostable\",BooleanType(),True),\\\n                      StructField(\"no_follow\",BooleanType(),True),\\\n                      StructField(\"num_comments\",LongType(),True),\\\n                      StructField(\"log_num_comments\",DoubleType(),True),\\\n                      StructField(\"over_18\",BooleanType(),True),\\\n                      StructField(\"subreddit_id\",StringType(),True),\\\n                      StructField(\"commentsUrl\",StringType(),True),\\\n                      StructField(\"whitelist_status\",StringType(),False),\\\n                      StructField(\"suggested_sort\",StringType(),False),\\\n                      StructField(\"titleClean\",StringType(),True),\\\n                      StructField(\"commentsClean\",StringType(),False),\\\n                      StructField(\"score\",LongType(),True),\\\n                      StructField(\"log_score\",DoubleType(),True),\n                      StructField(\"test\",BooleanType(),False)\\\n                    ])\n\n# Reading the file into a dataframe\n# 2 filepaths are stated below \n# - one for training data and initial test set\n# - one for training data and the OOT test set\n\n# Filepath for training tata and initial test set\nfilePath = \"dbfs:/FileStore/df/train_test_data.parquet\"\n\n# Filepath for training tata and final OOT test set\nfilePathOOT = \"dbfs:/FileStore/df/train_OOT_data.parquet\"\n\n# Choose the correct filepath from above to read into the dataframe\nclean_df = spark.read.format(\"parquet\").option(\"header\", \"true\").schema(schema).load(filePath)\n\n# Drop any row having any column as \"null\"\nclean_df = clean_df.na.drop(how=\"any\")\n\n# Separating training dataframe\ntrain_df = clean_df.where(col(\"test\")==False)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Statistical summary of number of comments and scores for training data set\n# Log transform applied to num_comments and score to reduce skewness of data\n# ==========================================================================\n\ntrain_df.select(\"num_comments\",\\\n                \"log_num_comments\",\\\n                \"score\",\\\n                \"log_score\").describe().show()\ntrain_df.agg(F.skewness(\"num_comments\"),\\\n             F.skewness(\"log_num_comments\"),\\\n             F.skewness(\"score\"),\\\n             F.skewness(\"log_score\")).show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Undersampling the majority data based on score \n# because a vast majority of the training data has a score of 0\n# =============================================================\n\n# Defining a function to undersample\ndef removeMajority(df, targetcol=\"scoreOutlier\", targetval=0.0, removal=0.9):\n  sample_df = df.where((col(targetcol)==targetval) & (col(\"test\")==False))\n  sample_df2 = df.where((col(targetcol)!=targetval) & (col(\"test\")==False))\n  sample_df = sample_df.sampleBy(col=targetcol, fractions = {targetval: 1-removal}, seed = 69)\n  sample_df2 = sample_df2.union(sample_df)\n  sample_df3 = df.where(col(\"test\")==True)\n  sample_df3 = sample_df3.union(sample_df2)\n  return sample_df3\n\n# Create a new training dataframe with majority undersampled\ntrain_df2 = removeMajority(df = train_df, targetcol = \"score\", targetval = 0, removal = 0.75)\n\n# Printing the change in number of training datapoints due to undersampling\nprint(\"No. of training datapoints before undersampling = {}\".format(train_df.agg(F.count(\"test\")).first()[0]))\nprint(\"No. of training datapoints after undersampling = {}\".format(train_df2.agg(F.count(\"test\")).first()[0]))\n\n# Printing the change in skewness of data as a result of undersampling\nprint(\"Skewness of log_score data before undersampling = {}\".format(train_df.agg(F.skewness(\"log_score\")).first()[0]))\nprint(\"Skewness of log_score data after undersampling = {}\".format(train_df2.agg(F.skewness(\"log_score\")).first()[0]))\nprint(\"Skewness of log_num_comments data before undersampling = {}\".format(train_df.agg(F.skewness(\"log_num_comments\")).first()[0]))\nprint(\"Skewness of log_num_comments data after undersampling = {}\".format(train_df2.agg(F.skewness(\"log_num_comments\")).first()[0]))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Display Training Data Before Undersampling for Plots\n# ====================================================\n\ndisplay(train_df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Display Training Data After Undersampling for Plots\n# ===================================================\n\ndisplay(train_df2)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Removing Outliers Using Z-Score\n# We are using the log_num_comments and log_score features to determine outliers\n# Datapoints in which both these feature values fall outside the mean +/- 3*std_dev range are discarded\n# =====================================================================================================\n\n# Computing quartiles and z-scores for the 2 chosen numerical data fields - log_score and log_num_comments\nfrom pyspark.sql.types import BooleanType\nqtlScore = train_df2.where(col(\"test\")==False).approxQuantile(\"log_score\", [0.00, 0.75], 0.0)\nqtlComments = train_df2.where(col(\"test\")==False).approxQuantile(\"log_num_comments\", [0.00, 0.75], 0.0)\nstdevScore, meanScore = train_df2.select(F.stddev(\"log_score\"), F.mean(\"log_score\")).first()\nstdevComments, meanComments = train_df2.select(F.stddev(\"log_num_comments\"), F.mean(\"log_num_comments\")).first()\n\n# Defining an outlier detection function based on z-score\ndef outlier(comments, score):\n  Outlier = (score>meanScore+(3*stdevScore)) and (comments>meanComments+(3*stdevComments))\n  if Outlier:\n    return True\n  else:\n    return False\noutlier_udf = udf(outlier, BooleanType())\n\n# Defining an outlier detection function based on IQR\ndef outlierIQR(comments, score):\n  Outlier = (score>qtlScore[1]) and (comments>qtlComments[1])\n  if Outlier:\n    return True\n  else:\n    return False\noutlierIQR_udf = udf(outlierIQR, BooleanType())\n\n# Computing outliers based on\ndf3 = train_df2.withColumn(\"Outlier\", outlier_udf(col(\"log_num_comments\"), col(\"log_score\")))\ndf3IQR = train_df2.withColumn(\"Outlier\", outlierIQR_udf(col(\"log_num_comments\"), col(\"log_score\")))\ndf4Zscore = df3.where(col(\"Outlier\")==False)# | (col(\"Outlier\")==True)) #can remove outliers if needed\n\n# Printing the change in number of training datapoints due to removal of outliers\nprint(\"Choosing Z-score to detect and eliminate outliers...\")\nprint(\"No. of training datapoints before dropping outliers = {}\".format(df3.agg(F.count(\"test\")).first()[0]))\nprint(\"No. of training datapoints after dropping outliers = {}\".format(df4Zscore.agg(F.count(\"test\")).first()[0]))\n\n# Printing the change in skewness of data as a result of dropping the outliers\nprint(\"Skewness of log_score data before dropping outliers = {}\".format(df3.agg(F.skewness(\"log_score\")).first()[0]))\nprint(\"Skewness of log_score data after dropping outliers = {}\".format(df4Zscore.agg(F.skewness(\"log_score\")).first()[0]))\nprint(\"Skewness of log_num_comments data before dropping outliers = {}\".format(df3.agg(F.skewness(\"log_num_comments\")).first()[0]))\nprint(\"Skewness of log_num_comments data after dropping outliers = {}\".format(df4Zscore.agg(F.skewness(\"log_num_comments\")).first()[0]))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Display Outliers Detected Based on Z-score\n# ==========================================\n\ndisplay(df3)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Display Outliers Detected Based on IQR\n# ======================================\n\ndisplay(df3IQR)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Splitting Training DataFrame into Training and Validation Sets\n# ==============================================================\n\n# Setting the ratio of training set\ntrainSplitRatio = 0.7\n\n# Splitting the majority data into training and validation sets\ntrain_set0, val_set0 = df4Zscore.where(col(\"score\")==0).randomSplit([trainSplitRatio, 1-trainSplitRatio], seed = 420)\n\n# Splitting the remaining (minority) data into training and validation sets\ntrain_set, val_set = df4Zscore.where(col(\"score\")!=0).randomSplit([trainSplitRatio, 1-trainSplitRatio], seed = 125)\n\n# Combining the majority and minority datasets to produce the final training and validation datasets\ntrain_dataset = train_set.union(train_set0).cache()\nval_dataset = val_set.union(val_set0).cache()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Additional Features Based on Window Functions\n# =============================================\n\n# Importing libraries\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import FloatType\n\n# Defining the respective windows\nwindow1 = Window.partitionBy(clean_df.subreddit_id)\nwindow2 = Window.partitionBy(clean_df.author)\n\n# Separating the test data into a new dataframe\ntest_dataset = clean_df.where(col(\"test\")==True)\n\n# Adding window features to training dataset\ntrain_dataset = train_dataset.withColumn(\"Avg_Comments_Subreddit\",F.avg(\"num_comments\").over(window1))\ntrain_dataset = train_dataset.withColumn(\"Sum_Comments_Subreddit\",F.sum(\"num_comments\").over(window1))\ntrain_dataset = train_dataset.withColumn(\"Avg_Comments_Author\",F.avg(\"num_comments\").over(window2))\ntrain_dataset = train_dataset.withColumn(\"Sum_Comments_Author\",F.sum(\"num_comments\").over(window2))\ntrain_dataset = train_dataset.withColumn(\"avg_is_crosspostable_subreddit\", F.avg(col(\"is_crosspostable\").cast(FloatType())).over(window1))\ntrain_dataset = train_dataset.withColumn(\"avg_is_crosspostable_author\", F.avg(col(\"is_crosspostable\").cast(FloatType())).over(window2))\ntrain_dataset = train_dataset.withColumn(\"avg_no_follow_subreddit\", F.avg(col(\"no_follow\").cast(FloatType())).over(window1))\ntrain_dataset = train_dataset.withColumn(\"avg_no_follow_author\", F.avg(col(\"no_follow\").cast(FloatType())).over(window2))\n\n# Adding window features to validation dataset\nval_dataset = val_dataset.withColumn(\"Avg_Comments_Subreddit\",F.avg(\"num_comments\").over(window1))\nval_dataset = val_dataset.withColumn(\"Sum_Comments_Subreddit\",F.sum(\"num_comments\").over(window1))\nval_dataset = val_dataset.withColumn(\"Avg_Comments_Author\",F.avg(\"num_comments\").over(window2))\nval_dataset = val_dataset.withColumn(\"Sum_Comments_Author\",F.sum(\"num_comments\").over(window2))\nval_dataset = val_dataset.withColumn(\"avg_is_crosspostable_subreddit\", F.avg(col(\"is_crosspostable\").cast(FloatType())).over(window1))\nval_dataset = val_dataset.withColumn(\"avg_is_crosspostable_author\", F.avg(col(\"is_crosspostable\").cast(FloatType())).over(window2))\nval_dataset = val_dataset.withColumn(\"avg_no_follow_subreddit\", F.avg(col(\"no_follow\").cast(FloatType())).over(window1))\nval_dataset = val_dataset.withColumn(\"avg_no_follow_author\", F.avg(col(\"no_follow\").cast(FloatType())).over(window2))\n\n# Adding window features to test dataset\ntest_dataset = test_dataset.withColumn(\"Avg_Comments_Subreddit\",F.avg(\"num_comments\").over(window1))\ntest_dataset = test_dataset.withColumn(\"Sum_Comments_Subreddit\",F.sum(\"num_comments\").over(window1))\ntest_dataset = test_dataset.withColumn(\"Avg_Comments_Author\",F.avg(\"num_comments\").over(window2))\ntest_dataset = test_dataset.withColumn(\"Sum_Comments_Author\",F.sum(\"num_comments\").over(window2))\ntest_dataset = test_dataset.withColumn(\"avg_is_crosspostable_subreddit\", F.avg(col(\"is_crosspostable\").cast(FloatType())).over(window1))\ntest_dataset = test_dataset.withColumn(\"avg_is_crosspostable_author\", F.avg(col(\"is_crosspostable\").cast(FloatType())).over(window2))\ntest_dataset = test_dataset.withColumn(\"avg_no_follow_subreddit\", F.avg(col(\"no_follow\").cast(FloatType())).over(window1))\ntest_dataset = test_dataset.withColumn(\"avg_no_follow_author\", F.avg(col(\"no_follow\").cast(FloatType())).over(window2))\n\n# Display training dataset\ndisplay(train_dataset)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Dropping Rows with [deleted] Authors from ALL datasets\ndf4 = train_dataset.where(col(\"author\")!=\"[deleted]\")\n\nprint(\"Rows with [deleted] authors removed from train dataset...\\n\")\nprint(\"Training datapoints before dropping [deleted] authors = {}\".format(train_dataset.agg(F.count(\"author\")).first()[0]))\nprint(\"Training datapoints after dropping [deleted] authors = {}\\n\".format(df4.agg(F.count(\"author\")).first()[0]))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Correlation Matrix to Eliminate Boolean / Numerical Columns\n# ===========================================================\n\n# Importing libraries\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.stat import Correlation\n\n# Columns by each type\nallNumericColumns = [\\\n                     'log_num_comments',\\\n                    ]\nallBooleanColumns = [\\\n                     'author_cakeday', \\\n                     'brand_safe', \\\n                     'is_crosspostable', \\\n                     'no_follow', \\\n                     'can_gild', \\\n                     'over_18', \\\n                    ]\n\ncorr_features = allNumericColumns + allBooleanColumns\n\ndfcorr = train_df.select(corr_features)\n\n# convert to vector column first\nvector_col = \"corr_features\"\nassembler = VectorAssembler(inputCols=dfcorr.columns, outputCol=vector_col)\ndf_vector = assembler.transform(dfcorr).select(vector_col)\n\nmatrix = Correlation.corr(df_vector, vector_col).collect()[0][0]\ncorrmatrix = matrix.toArray().tolist()\ndfcorrcoefficient = spark.createDataFrame(corrmatrix,corr_features)\ndisplay(dfcorrcoefficient)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Transforming all datasets into feature vectors\n# ==============================================\n\n# Importing libraries\nfrom pyspark.ml.feature import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType, ArrayType\nimport string\nimport re\n\n# Copying all datasets into new dataframes\ndf5 = df4\ndf5v = val_dataset\ndf5_t = test_dataset\n\n# Defining the target variable\nlabelColumn = 'log_score'\n\n# Selected boolean features\nbooleanColumns = [\\\n                  'author_cakeday', \\\n                  'brand_safe', \\\n                  'is_crosspostable', \\\n                  'no_follow', \\\n                  'can_gild', \\\n                 ]\n\n# Selected categorical features\ncategoricalColumns = [\\\n                      'domain', \\\n                      'subreddit_id', \\\n                      'whitelist_status', \\\n                      'suggested_sort',\\\n                     ]\n\n# Selected and boolean-derived numeric features\nnumericColumns = [\\\n                  'log_num_comments',\\\n                  'created_utc_day',\\\n                  'created_utc_hr',\\\n                  'Avg_Comments_Author',\\\n                  'Avg_Comments_Subreddit',\\\n                  'avg_is_crosspostable_subreddit',\\\n                  'avg_is_crosspostable_author',\\\n                  'avg_no_follow_subreddit',\\\n                  'avg_no_follow_author',\\\n                 ]\n\n# Selected text features\ntextColumns = [\\\n               \"titleClean\", \\\n               \"commentsClean\",\\\n              ]\n\n# Scaling the selected date features to a range of (0,1]\ndateFields = {'created_utc_day': 31.0, 'created_utc_hr': 24.0}\ndateColumns = []\nfor dateField in dateFields:\n  df5 = df5.withColumn(dateField+\"Scaled\", (1.0*col(dateField))/dateFields[dateField])\n  df5v = df5v.withColumn(dateField+\"Scaled\", (1.0*col(dateField))/dateFields[dateField])\n  df5_t = df5_t.withColumn(dateField+\"Scaled\", (1.0*col(dateField))/dateFields[dateField])\n  dateColumns.append(dateField+\"Scaled\")\n\n# Initialize transformation pipeline stages\nstages = []\n\n# Encoding boolean features\nfor boolCol in booleanColumns:\n  df5 = df5.withColumn(boolCol+\"_str\", col(boolCol).cast(StringType()))\n  df5v = df5v.withColumn(boolCol+\"_str\", col(boolCol).cast(StringType())) \n  df5_t = df5_t.withColumn(boolCol+\"_str\", col(boolCol).cast(StringType())) \n  stringIndexer0 = StringIndexer(inputCol = boolCol+\"_str\", outputCol = boolCol + 'Index').setHandleInvalid(\"keep\")\n  encoder0 = OneHotEncoder(inputCols = [stringIndexer0.getOutputCol()], \\\n                                   outputCols = [boolCol + \"classVec\"])\n  stages += [stringIndexer0, encoder0]\n\n# Encoding categorical features\nfor categoricalCol in categoricalColumns:\n  stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index').setHandleInvalid(\"keep\")\n  encoder = OneHotEncoder(inputCols = [stringIndexer.getOutputCol()], \\\n                          outputCols = [categoricalCol + \"classVec\"])\n  stages += [stringIndexer, encoder]\n\n# Encoding text features\nfor textCol in textColumns:\n  tokenizer = Tokenizer(inputCol = textCol, outputCol = textCol + 'words')\n  stopwords = StopWordsRemover().setInputCol(textCol + \"words\").setOutputCol(textCol+\"filtered\")\n  #ngram = NGram(n=3, inputCol=stopwords.getOutputCol(), outputCol=textCol+\"Ngrams\")\n  hashtf = HashingTF(numFeatures = 500, inputCol = textCol+\"filtered\", #2**16 \\\n                     outputCol = textCol + 'tf')\n  idf = IDF(inputCol = hashtf.getOutputCol(), outputCol = textCol + 'idf', minDocFreq = 1)\n  word_embeddings = Word2Vec(vectorSize=100, minCount=0, inputCol=textCol+\"filtered\", outputCol=textCol+\"Embeddings\")\n  stages += [tokenizer, stopwords, hashtf, idf, word_embeddings]\n\n# Building feature extraction pipeline and extracting features\npipeline = Pipeline(stages = stages)\npipelineModel = pipeline.fit(df5)\ntrain_dataset = pipelineModel.transform(df5)\n\n# Selecting top features from categorical columns using chi-square selector\nnum_features = 45\ncategoricalColumnsVec = [c + 'classVec' for c in categoricalColumns]\nassembler_cat = VectorAssembler(inputCols = categoricalColumnsVec, outputCol = 'categoricalColumnsVecBefore')\nselector_cat = ChiSqSelector(numTopFeatures=num_features, \\\n\t\t\t\t\t\t\t featuresCol=\"categoricalColumnsVecBefore\", \\\n\t\t\t\t\t\t\t outputCol=\"categoricalColumnsVecAfter\", \\\n\t\t\t\t\t\t\t labelCol=labelColumn)\ncat_selection = Pipeline(stages = [assembler_cat, selector_cat])\ncat_selection_model = cat_selection.fit(train_dataset)\ntrain_dataset = cat_selection_model.transform(train_dataset)\n\n# Removal of non ASCII characters from text features\ntextFeats = [\"title\", \"comments\"]\ndef strip_non_ascii(data_str):\n  # Returns the string without non ASCII characters\n  stripped = (c for c in data_str if 0 < ord(c) < 127)\n  return ''.join(stripped)\n# setup pyspark udf function\nstrip_non_ascii_udf = udf(strip_non_ascii, StringType())\nfor text in textFeats:\n  train_dataset = train_dataset.withColumn(text+'_non_asci',strip_non_ascii_udf(train_dataset[text+'Clean']))\n\n# Fixing of abbreviations in text features\ndef fix_abbreviation(data_str):\n  data_str = data_str.lower()\n  data_str = re.sub(r'\\bthats\\b', 'that is', data_str)\n  data_str = re.sub(r'\\bits\\b', 'it is', data_str)\n  data_str = re.sub(r'\\bhes\\b', 'he is', data_str)\n  data_str = re.sub(r'\\bshes\\b', 'she is', data_str)\n  data_str = re.sub(r'\\btheyre\\b', 'they are', data_str)\n  data_str = re.sub(r'\\bthatd\\b', 'that would', data_str)\n  data_str = re.sub(r'\\bive\\b', 'i have', data_str)\n  data_str = re.sub(r'\\bim\\b', 'i am', data_str)\n  data_str = re.sub(r'\\bya\\b', 'yeah', data_str)\n  data_str = re.sub(r'\\bcant\\b', 'can not', data_str)\n  data_str = re.sub(r'\\bdont\\b', 'do not', data_str)\n  data_str = re.sub(r'\\bwont\\b', 'will not', data_str)\n  data_str = re.sub(r'\\bisnt\\b', 'is not', data_str)\n  data_str = re.sub(r'\\barent\\b', 'are not', data_str)\n  data_str = re.sub(r'\\bwerent\\b', 'were not', data_str)\n  data_str = re.sub(r'\\bwouldnt\\b', 'would not', data_str)\n  data_str = re.sub(r'\\bshouldnt\\b', 'should not', data_str)\n  data_str = re.sub(r'\\bcouldnt\\b', 'could not', data_str)\n  data_str = re.sub(r'\\bwasnt\\b', 'was not', data_str)\n  data_str = re.sub(r'\\byoure\\b', 'you are', data_str)\n  data_str = re.sub(r'\\bid\\b', 'i would', data_str)\n  data_str = re.sub(r'wtf', 'what the fuck', data_str)\n  data_str = re.sub(r'\\bwth\\b', 'what the hell', data_str)\n  data_str = re.sub(r'\\bomg\\b', 'oh my god', data_str)\n  data_str = re.sub(r'\\blol\\b', 'laughing', data_str)\n  data_str = re.sub(r'\\brofl\\b', 'laughing', data_str)\n  data_str = re.sub(r'\\blmao\\b', 'laughing', data_str)\n  data_str = re.sub(r'\\br\\b', 'are', data_str)\n  data_str = re.sub(r'\\bu\\b', 'you', data_str)\n  data_str = re.sub(r'\\bur\\b', 'your', data_str)\n  data_str = re.sub(r'\\bk\\b', 'OK', data_str)\n  data_str = re.sub(r'\\bsux\\b', 'sucks', data_str)\n  data_str = re.sub(r'\\bno+\\b', 'no', data_str)\n  data_str = re.sub(r'\\bcoo+\\b', 'cool', data_str)\n  data_str = re.sub(r'rt\\b', '', data_str)\n  data_str = data_str.strip()\n  return data_str\nfix_abbreviation_udf = udf(fix_abbreviation, StringType())\nfor text in textFeats:\n  train_dataset = train_dataset.withColumn(text+'_fixed_abbrev',fix_abbreviation_udf(train_dataset[text+'_non_asci']))\n\n# Removal of features irrelevant to sentiment analysis\ndef remove_features(data_str):\n  # compile regex\n  url_re = re.compile('https?://(www.)?\\w+\\.\\w+(/\\w+)*/?')\n  punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n  num_re = re.compile('(\\\\d+)')\n  mention_re = re.compile('@(\\w+)')\n  alpha_num_re = re.compile(\"^[a-z0-9_.]+$\")\n  # convert to lowercase\n  data_str = data_str.lower()\n  # remove hyperlinks\n  data_str = url_re.sub(' ', data_str)\n  # remove @mentions\n  data_str = mention_re.sub(' ', data_str)\n  # remove puncuation\n  data_str = punc_re.sub(' ', data_str)\n  # remove numeric 'words'\n  data_str = num_re.sub(' ', data_str)\n  # remove non a-z 0-9 characters and words shorter than 1 characters\n  list_pos = 0\n  cleaned_str = ''\n  for word in data_str.split():\n    if list_pos == 0:\n      if alpha_num_re.match(word) and len(word) > 1:\n        cleaned_str = word\n      else:\n        cleaned_str = ' '\n    else:\n      if alpha_num_re.match(word) and len(word) > 1:\n        cleaned_str = cleaned_str + ' ' + word\n      else:\n        cleaned_str += ' '\n    list_pos += 1\n    # remove unwanted space, *.split() will automatically split on\n    # whitespace and discard duplicates, the \" \".join() joins the\n    # resulting list into one string.\n  return \" \".join(cleaned_str.split())\nremove_features_udf = udf(remove_features, StringType())\nfor text in textFeats:\n  train_dataset = train_dataset.withColumn(text+'_removed',remove_features_udf(train_dataset[text+'_fixed_abbrev']))\n\n# Sentiemnt analysis function\nfrom pyspark.sql.types import FloatType\nfrom textblob import TextBlob\n\ndef sentiment_analysis(text):\n  value = TextBlob(text).sentiment.polarity\n  if -0.1<value<=0.1:\n    value = 0.0\n  elif value>0.1:\n    value = 1.0\n  else:\n    value = -1.0\n  return value\nsentiment_analysis_udf = udf(sentiment_analysis , FloatType())\nfor text in textFeats:\n  train_dataset  = train_dataset.withColumn(text+'_sentiment_score', sentiment_analysis_udf(train_dataset[text+'_removed'] ))\n\n# Calculating length of title  \ntrain_dataset = train_dataset.withColumn(\"titleLength\", F.log(1+F.size(col(\"titleCleanfiltered\"))))\n\n# Calculating average comment length\ndef commentLength(num, comment):\n  if num>0:\n    length = len(comment)\n    avglength = 1.0*length/num\n  else:\n    avglength = 0.0\n  return avglength\ncommentLength_udf = udf(commentLength, FloatType())\ntrain_dataset = train_dataset.withColumn(\"commentLength\", F.log(1+commentLength_udf(col(\"num_comments\"), col(\"commentsClean\"))))\n\n# Assembling all features into a vector for the training set\nassemblerInputs =  [\"categoricalColumnsVecAfter\"] \\\n+ [b + 'classVec' for b in booleanColumns] \\\n+ numericColumns \\\n+ dateColumns\\\n+ ['title_sentiment_score', 'titleLength'] \\\n+ ['comments_sentiment_score', 'commentLength'] \\\n+ [t + 'Embeddings' for t in textColumns]\nassembler = VectorAssembler(inputCols = assemblerInputs, outputCol = 'features')\ntrain_dataset = assembler.transform(train_dataset)\n\n# Selecting train dataset only where the title at least has a meaningful word\ntrain_dataset = train_dataset.where((F.size(col('titleCleanfiltered'))>1))\ntrain_set = train_dataset.select('features', labelColumn)\ntrain_set = train_set.cache()\n\n# Fitting trained pipelines on validation and test datasets\nval_set = pipelineModel.transform(df5v)\nval_set = cat_selection_model.transform(val_set)\ntest_dataset = pipelineModel.transform(df5_t)\ntest_dataset = cat_selection_model.transform(test_dataset)\n\n# remove non ASCII characters from validation and test sets\nval_set = val_set.withColumn('title_non_asci',strip_non_ascii_udf(val_set['titleClean']))\nval_set = val_set.withColumn('comments_non_asci',strip_non_ascii_udf(val_set['commentsClean']))\ntest_dataset = test_dataset.withColumn('title_non_asci',strip_non_ascii_udf(test_dataset['titleClean']))\ntest_dataset = test_dataset.withColumn('comments_non_asci',strip_non_ascii_udf(test_dataset['commentsClean']))\n\n# fixed abbreviation from validation and test sets\nval_set = val_set.withColumn('title_fixed_abbrev',fix_abbreviation_udf(val_set['title_non_asci']))\nval_set = val_set.withColumn('comments_fixed_abbrev',fix_abbreviation_udf(val_set['comments_non_asci']))\ntest_dataset = test_dataset.withColumn('title_fixed_abbrev',fix_abbreviation_udf(test_dataset['title_non_asci']))\ntest_dataset = test_dataset.withColumn('comments_fixed_abbrev',fix_abbreviation_udf(test_dataset['comments_non_asci']))\n\n# remove features irrelevant to sentiment analysis from validation and test sets\nval_set = val_set.withColumn('title_removed',remove_features_udf(val_set['title_fixed_abbrev']))\nval_set = val_set.withColumn('comments_removed',remove_features_udf(val_set['comments_fixed_abbrev']))\ntest_dataset = test_dataset.withColumn('title_removed',remove_features_udf(test_dataset['title_fixed_abbrev']))\ntest_dataset = test_dataset.withColumn('comments_removed',remove_features_udf(test_dataset['comments_fixed_abbrev']))\n\n# sentiemnt analysis on validation and test sets\nval_set  = val_set.withColumn(\"title_sentiment_score\", sentiment_analysis_udf(val_set['title_removed'] ))\nval_set  = val_set.withColumn(\"comments_sentiment_score\", sentiment_analysis_udf(val_set['comments_removed'] ))\nval_set = val_set.withColumn(\"titleLength\", F.log(1+F.size(col(\"titleCleanfiltered\"))))\ntest_dataset  = test_dataset.withColumn(\"title_sentiment_score\", sentiment_analysis_udf(test_dataset['title_removed'] ))\ntest_dataset  = test_dataset.withColumn(\"comments_sentiment_score\", sentiment_analysis_udf(test_dataset['comments_removed'] ))\ntest_dataset = test_dataset.withColumn(\"titleLength\", F.log(1+F.size(col(\"titleCleanfiltered\"))))\n\n# Average comment length for validation and test sets\nval_set = val_set.withColumn(\"commentLength\", F.log(1+commentLength_udf(col(\"num_comments\"), col(\"commentsClean\"))))\ntest_dataset = test_dataset.withColumn(\"commentLength\", F.log(1+commentLength_udf(col(\"num_comments\"), col(\"commentsClean\"))))\n\n# Assembling features into a vector for validation and test sets\nval_set = assembler.transform(val_set)\nval_set.cache()\ntest_dataset = assembler.transform(test_dataset)\ntest_dataset.cache()\n\nval_set = val_set.select('features', labelColumn)\nval_set = val_set.cache()\ntest_set = test_dataset.select('features', labelColumn)\ntest_set = test_set.cache()\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Displaying Training Dataset for Plotting\n# ========================================\n\ndisplay(train_dataset)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["train_set.show(5)\nval_set.show(5)\ntest_set.show(5)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Defining Performance Measurement Functions\n# ==========================================\n\n# Importing libraries\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Root mean square error\ndef rmse(df, predCol=\"predictedScore\", actCol=\"actualScore\"):\n  zero = df.where(col(actCol)==0)\n  nonzero = df.where(col(actCol)!=0)\n  evaluator = RegressionEvaluator(labelCol=actCol, predictionCol=predCol, metricName=\"rmse\")\n  rmse_total = evaluator.evaluate(df)\n  rmse_zero = evaluator.evaluate(zero)\n  rmse_nonzero = evaluator.evaluate(nonzero)\n  return [rmse_total, rmse_zero, rmse_nonzero]\n\n# Symmetric mean absolute percentage error\ndef smape(df, predCol=\"predictedScore\", actCol=\"actualScore\"):\n  zero = df.where(col(actCol)==0)\n  nonzero = df.where(col(actCol)!=0)\n  sm = df.withColumn(\"sm\", 100*((F.abs(col(predCol) - col(actCol)))/(0.5*(F.abs(col(actCol))+F.abs(col(predCol))))))\n  sm_zero = zero.withColumn(\"sm\", 100*((F.abs(col(predCol) - col(actCol)))/(0.5*(F.abs(col(actCol))+F.abs(col(predCol))))))\n  sm_nonzero = nonzero.withColumn(\"sm\", 100*((F.abs(col(predCol) - col(actCol)))/(0.5*(F.abs(col(actCol))+F.abs(col(predCol))))))\n  smape_total = sm.agg(F.mean(\"sm\")).first()[0]\n  smape_zero = sm_zero.agg(F.mean(\"sm\")).first()[0]\n  smape_nonzero = sm_nonzero.agg(F.mean(\"sm\")).first()[0]\n  return [smape_total, smape_zero, smape_nonzero]\n\n# Mean average error\ndef mae(df, predCol=\"predictedScore\", actCol=\"actualScore\"):\n  zero = df.where(col(actCol)==0)\n  nonzero = df.where(col(actCol)!=0)\n  evaluator = RegressionEvaluator(labelCol=actCol, predictionCol=predCol, metricName=\"mae\")\n  mae_total = evaluator.evaluate(df)\n  mae_zero = evaluator.evaluate(zero)\n  mae_nonzero = evaluator.evaluate(nonzero)\n  return [mae_total, mae_zero, mae_nonzero]\n\n# Lightweight regression evaluators\nevaluatorR2 = RegressionEvaluator(labelCol=\"log_score\", predictionCol=\"prediction\", metricName=\"r2\")\nevaluatorMAE = RegressionEvaluator(labelCol=\"actualScore\", predictionCol=\"predictedScore\", metricName=\"mae\")"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Training Models for Manual Hyperparameter Search\n# Due to Cluster Limitations [initializing results list]\n# ======================================================\n\nresults_rf = []\nresults_gbt = []\ntest_results = []"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# Training RF Model for Manual Hyperparameter Search\n# Due to Cluster Limitations [training the model]\n# ==================================================\n\n# Importing libraries\nfrom pyspark.ml.regression import RandomForestRegressor\n\n# Hyperparameters\nnum_trees = 13\nmax_depth = 6\n\nprint(\"Training Random Forest Regressor Model...\")\nrf = RandomForestRegressor(featuresCol=\"features\", \\\n\t\t\t\t\t\t   labelCol=\"log_score\", \\\n\t\t\t\t\t\t   numTrees=num_trees, \\\n\t\t\t\t\t\t   minInstancesPerNode=5, \\\n\t\t\t\t\t\t   maxDepth=max_depth, \\\n\t\t\t\t\t\t   minInfoGain=0.00, \\\n\t\t\t\t\t\t   featureSubsetStrategy=\"auto\") \n\nmodel_rf = rf.fit(train_set)\nprint(\"Model Trained\")\n\n# Transform validation data\npredictions = model_rf.transform(val_set)\noutput = predictions.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput = output.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Validation Data Transformed\")\n\n# Transform training data\npredictions_tr = model_rf.transform(train_set)\noutput_tr = predictions_tr.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_tr = output_tr.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Training Data Transformed\")\n\n# Printing errors\nmae_train = evaluatorMAE.evaluate(output_tr)\nprint(\"Train MAE = {}\".format(mae_train))\nmae_val = evaluatorMAE.evaluate(output)\nprint(\"Validation MAE = {}\".format(mae_val))\n\n# Append results to list\nresults_rf.append((num_trees, max_depth, mae_train, mae_val))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# Training RF Model for Manual Hyperparameter Search\n# Due to Cluster Limitations [plotting results - Trees]\n# =====================================================\n\nresults_rfdf = spark.createDataFrame(results_rf, [\"numTrees\", \"maxDepth\", \"MAE_train\", \"MAE_val\"])\ndisplay(results_rfdf.where(col(\"maxDepth\")==5))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Training RF Model for Manual Hyperparameter Search\n# Due to Cluster Limitations [plotting results - Depth]\n# =====================================================\n\nresults_rfdf = spark.createDataFrame(results_rf, [\"numTrees\", \"maxDepth\", \"MAE_train\", \"MAE_val\"])\ndisplay(results_rfdf.where(col(\"numTrees\")==13))"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Apply Tuned RF Model to Test Data\n# =================================\n\n# Transform test data\npredictions_tst = model_rf.transform(test_set)\noutput_tst = predictions_tst.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_tst = output_tst.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Test Data Transformed\")\n\n# Printing errors\nmae_test = mae(output_tst)\ntest_results.append((\"Random Forest\", mae_test[0], mae_test[1], mae_test[2]))\nprint(\"Test MAE = {}\".format(mae_test))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Training GBT Model for Manual Hyperparameter Search\n# Due to Cluster Limitations\n# ===================================================\n\n# Importing libraries\nfrom pyspark.ml.regression import GBTRegressor\n\n# Hyperparameters\nmaxIter = 10\nmaxDepth = 4\n\nprint(\"Training Gradient Boosted Trees Regressor Model...\")\ngbt = GBTRegressor(\\\n                   featuresCol=\"features\",\\\n                   labelCol=\"log_score\",\\\n                   maxIter=maxIter,\\\n                   maxDepth=maxDepth,\\\n                  )\n\nmodel_gbt = gbt.fit(train_set)\nprint(\"Model Trained\")\n\n# Transform validation data\npredictions_gb = model_gbt.transform(val_set)\noutput_gb = predictions_gb.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_gb = output_gb.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Validation Data Transformed\")\n\n# Transform training data\npredictions_gbtr = model_gbt.transform(train_set)\noutput_gbtr = predictions_gbtr.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_gbtr = output_gbtr.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Training Data Transformed\")\n\n# Error computation\n# model_dict[\"rmse_val\"] = rmse(output)\nmae_valid = evaluatorMAE.evaluate(output_gb)\nprint(\"Validation MAE = {}\".format(mae_valid))\nmae_train = evaluatorMAE.evaluate(output_gbtr)\nprint(\"Training MAE = {}\".format(mae_train))\n\n# Append results to list\nresults_gbt.append((maxIter, maxDepth, mae_train, mae_valid))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# Training GBT Model for Manual Hyperparameter Search\n# Due to Cluster Limitations [plotting results - maxIter]\n# =======================================================\n\nresults_gbtdf = spark.createDataFrame(results_gbt, [\"maxIter\", \"maxDepth\", \"MAE_train\", \"MAE_val\"])\ndisplay(results_gbtdf.where(col(\"maxDepth\")==1))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Training GBT Model for Manual Hyperparameter Search\n# Due to Cluster Limitations [plotting results - Depth]\n# =====================================================\n\nresults_gbtdf = spark.createDataFrame(results_gbt, [\"maxIter\", \"maxDepth\", \"MAE_train\", \"MAE_val\"])\ndisplay(results_gbtdf.where(col(\"maxIter\")==10))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# Apply Tuned GBT Model to Test Data\n# ==================================\n\n# Transform test data\npredictions_gbtst = model_gbt.transform(test_set)\noutput_gbtst = predictions_gbtst.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_gbtst = output_gbtst.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Test Data Transformed\")\n\n# Computing errors\nmae_test = mae(output_gbtst)\ntest_results.append((\"Gradient Boosted Trees\", mae_test[0], mae_test[1], mae_test[2]))\nprint(\"Test MAE = {}\".format(mae_test))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Displaying test results for plotting\n# ====================================\n\nresults_testdf = spark.createDataFrame(test_results, [\"Model\", \"MAE_total\", \"MAE_zero\", \"MAE_nonzero\"])\ndisplay(results_testdf)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# Hyperparameter Tuning for RF Model Using Parameter Grid Search\n# ==============================================================\n\n# Importing Libraries\nfrom pyspark.ml.regression import RandomForestRegressor\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n\n# Instantiating an RF model\nrf = RandomForestRegressor(featuresCol=\"features\", \\\n                           labelCol=\"log_score\", \\\n                           minInstancesPerNode=5, \\\n                           minInfoGain=0.00, \\\n                           featureSubsetStrategy=\"auto\")\n\n# Setting parameters for grid search\nnum_trees = [11,12,13]\nmax_depth = [7,8,9]\nparamGrid = ParamGridBuilder()\\\n            .addGrid(rf.numTrees, num_trees) \\\n            .addGrid(rf.maxDepth, max_depth)\\\n            .build()\n\n# Train-validation split\ntvs = TrainValidationSplit(estimator=rf,\\\n                           estimatorParamMaps=paramGrid,\\\n                           parallelism=10,\\\n                           evaluator=evaluatorMAE,\\\n                           trainRatio=0.70)\n\nmodel_rf = tvs.fit(train_set.union(val_set))\n\n# Best model parameters\nbest_rfmodel = model_rf.bestModel\nparam_dict = best_rfmodel.stages[-1].extractParamMap()\nprint(\"Tuned Model Parameters:\")\nhyperparameters = {}\nfor k, v in param_dict.items():\n  hyperparameters[k.name] = v\n  print(\"Hyper-parameter {} = {}\".format(k.name, hyperparameters[k.name]))\n\n# Transform test data\npredictions_tst = best_rfmodel.transform(test_set)\noutput_tst = predictions_tst.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_tst = output_tst.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Test Data Transformed\")\nmae_tst = mae(output_tst)\nprint(\"Test MAE (total): {}\".format(mae_tst[0]))\nprint(\"Test MAE (zero): {}\".format(mae_tst[1]))\nprint(\"Test MAE (nonzero): {}\\n\".format(mae_tst[2]))\n\n# Transform training data\npredictions_tr = best_rfmodel.transform(train_set.union(val_set))\noutput_tr = predictions_tr.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_tr = output_tr.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Training Data Transformed\")\nmae_tr = mae(output_tr)\nprint(\"Training MAE (total): {}\".format(mae_tr[0]))\nprint(\"Training MAE (zero): {}\".format(mae_tr[1]))\nprint(\"Training MAE (nonzero): {}\\n\".format(mae_tr[2]))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["# Hyperparameter Tuning for GBT Model Using Parameter Grid Search\n# ==============================================================+\n\n# Importing Libraries\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n\n# Instantiating an RF model\ngbt = RandomForestRegressor(featuresCol=\"features\", \\\n                            labelCol=\"log_score\", \\\n                            minInstancesPerNode=5)\n\n# Setting parameters for grid search\nmax_iter = [11,12,13]\nmax_depth = [2,3,4]\nparamGrid = ParamGridBuilder()\\\n            .addGrid(gbt.maxIter, max_iter) \\\n            .addGrid(gbt.maxDepth, max_depth)\\\n            .build()\n\n# Train-validation split\ntvs = TrainValidationSplit(estimator=gbt,\\\n                           estimatorParamMaps=paramGrid,\\\n                           parallelism=10,\\\n                           evaluator=evaluatorMAE,\\\n                           trainRatio=0.70)\n\nmodel_gbt = tvs.fit(train_set.union(val_set))\n\n# Best model parameters\nbest_gbtmodel = model_gbt.bestModel\nparam_dict = best_gbtmodel.stages[-1].extractParamMap()\nprint(\"Tuned Model Parameters:\")\nhyperparameters = {}\nfor k, v in param_dict.items():\n  hyperparameters[k.name] = v\n  print(\"Hyper-parameter {} = {}\".format(k.name, hyperparameters[k.name]))\n\n# Transform test data\npredictions_tst = best_gbtmodel.transform(test_set)\noutput_tst = predictions_tst.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_tst = output_tst.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Test Data Transformed\")\nmae_tst = mae(output_tst)\nprint(\"Test MAE (total): {}\".format(mae_tst[0]))\nprint(\"Test MAE (zero): {}\".format(mae_tst[1]))\nprint(\"Test MAE (nonzero): {}\\n\".format(mae_tst[2]))\n\n# Transform training data\npredictions_tr = best_gbtmodel.transform(train_set.union(val_set))\noutput_tr = predictions_tr.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_tr = output_tr.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Training Data Transformed\")\nmae_tr = mae(output_tr)\nprint(\"Training MAE (total): {}\".format(mae_tr[0]))\nprint(\"Training MAE (zero): {}\".format(mae_tr[1]))\nprint(\"Training MAE (nonzero): {}\\n\".format(mae_tr[2]))"],"metadata":{},"outputs":[],"execution_count":32}],"metadata":{"name":"Mie1628_project","notebookId":2157996069019465},"nbformat":4,"nbformat_minor":0}
