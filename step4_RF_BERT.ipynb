{"cells":[{"cell_type":"code","source":["# Configuration for sparknlp - third party library\n# # # Use DBR 6.5 ML and spark 2.4.5\n# # # Install this on maven: com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.4\n# # # Install spark-nlp jar on PYPI\n# ============================="],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Importing Necessary Libraries\n# =============================\n\nimport pyspark\nfrom pyspark.sql.functions import col\nfrom pyspark.sql import functions as F, SparkSession\nfrom pyspark.sql.types import FloatType\n\n\nimport pyspark.sql.functions as F\nimport pyspark.sql.types as T\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.sql.functions import udf\n\n\nimport sparknlp\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.pretrained import PretrainedPipeline"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Loading DataFrame from Pre-saved Parquet File\n# =============================================\n\n# Importing libraries\nfrom pyspark.sql.types import *\n\n# Defining the schema\nschema = StructType([\\\n                      StructField(\"author\",StringType(),True),\\\n                      StructField(\"author_cakeday\",BooleanType(),False),\\\n                      StructField(\"created_utc_day\",IntegerType(),True),\\\n                      StructField(\"created_utc_hr\",IntegerType(),True),\\\n                      StructField(\"brand_safe\",BooleanType(),True),\\\n                      StructField(\"can_gild\",BooleanType(),True),\\\n                      StructField(\"domain\",StringType(),True),\\\n                      StructField(\"is_crosspostable\",BooleanType(),True),\\\n                      StructField(\"no_follow\",BooleanType(),True),\\\n                      StructField(\"num_comments\",LongType(),True),\\\n                      StructField(\"log_num_comments\",DoubleType(),True),\\\n                      StructField(\"over_18\",BooleanType(),True),\\\n                      StructField(\"subreddit_id\",StringType(),True),\\\n                      StructField(\"commentsUrl\",StringType(),True),\\\n                      StructField(\"whitelist_status\",StringType(),False),\\\n                      StructField(\"suggested_sort\",StringType(),False),\\\n                      StructField(\"titleClean\",StringType(),True),\\\n                      StructField(\"commentsClean\",StringType(),False),\\\n                      StructField(\"score\",LongType(),True),\\\n                      StructField(\"log_score\",DoubleType(),True),\n                      StructField(\"test\",BooleanType(),False)\\\n                    ])\n\n# Reading the file into a dataframe\n# 2 filepaths are stated below \n# - one for training data and initial test set\n# - one for training data and the OOT test set\n\n# Filepath for training tata and initial test set\n# filePath = \"dbfs:/FileStore/tables/df/train_test_data.parquet\"\n\n# Filepath for training tata and final OOT test set\nfilePathOOT = \"dbfs:/FileStore/tables/df/train_OOT_data.parquet\"\n\n# Choose the correct filepath from above to read into the dataframe\nclean_df = spark.read.format(\"parquet\").option(\"header\", \"true\").schema(schema).load(filePathOOT)\n\n# Drop any row having any column as \"null\"\nclean_df = clean_df.na.drop(how=\"any\")\n\n# Additional drop of rows as the focus is on the wordembeddings of title.\nclean_df = clean_df.where(F.length(\"titleClean\")!=0)\n\n# Separating training dataframe\ntrain_df = clean_df.where(col(\"test\")==False)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Undersampling the majority data based on score \n# because a vast majority of the training data has a score of 0\n# =============================================================\n\n# Defining a function to undersample\ndef removeMajority(df, targetcol=\"scoreOutlier\", targetval=0.0, removal=0.9):\n  sample_df = df.where((col(targetcol)==targetval) & (col(\"test\")==False))\n  sample_df2 = df.where((col(targetcol)!=targetval) & (col(\"test\")==False))\n  sample_df = sample_df.sampleBy(col=targetcol, fractions = {targetval: 1-removal}, seed = 69)\n  sample_df2 = sample_df2.union(sample_df)\n  sample_df3 = df.where(col(\"test\")==True)\n  sample_df3 = sample_df3.union(sample_df2)\n  return sample_df3\n\n# Create a new training dataframe with majority undersampled\ntrain_df2 = removeMajority(df = train_df, targetcol = \"score\", targetval = 0, removal = 0.75)\n\n# Printing the change in number of training datapoints due to undersampling\nprint(\"No. of training datapoints before undersampling = {}\".format(train_df.agg(F.count(\"test\")).first()[0]))\nprint(\"No. of training datapoints after undersampling = {}\".format(train_df2.agg(F.count(\"test\")).first()[0]))\n\n# Printing the change in skewness of data as a result of undersampling\nprint(\"Skewness of log_score data before undersampling = {}\".format(train_df.agg(F.skewness(\"log_score\")).first()[0]))\nprint(\"Skewness of log_score data after undersampling = {}\".format(train_df2.agg(F.skewness(\"log_score\")).first()[0]))\nprint(\"Skewness of log_num_comments data before undersampling = {}\".format(train_df.agg(F.skewness(\"log_num_comments\")).first()[0]))\nprint(\"Skewness of log_num_comments data after undersampling = {}\".format(train_df2.agg(F.skewness(\"log_num_comments\")).first()[0]))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["# Removing Outliers Using Z-Score\n# We are using the log_num_comments and log_score features to determine outliers\n# Datapoints in which both these feature values fall outside the mean +/- 3*std_dev range are discarded\n# =====================================================================================================\n\n# Computing quartiles and z-scores for the 2 chosen numerical data fields - log_score and log_num_comments\nfrom pyspark.sql.types import BooleanType\nqtlScore = train_df2.where(col(\"test\")==False).approxQuantile(\"log_score\", [0.00, 0.75], 0.0)\nqtlComments = train_df2.where(col(\"test\")==False).approxQuantile(\"log_num_comments\", [0.00, 0.75], 0.0)\nstdevScore, meanScore = train_df2.select(F.stddev(\"log_score\"), F.mean(\"log_score\")).first()\nstdevComments, meanComments = train_df2.select(F.stddev(\"log_num_comments\"), F.mean(\"log_num_comments\")).first()\n\n# Defining an outlier detection function based on z-score\ndef outlier(comments, score):\n  Outlier = (score>meanScore+(3*stdevScore)) and (comments>meanComments+(3*stdevComments))\n  if Outlier:\n    return True\n  else:\n    return False\noutlier_udf = udf(outlier, BooleanType())\n\n# Defining an outlier detection function based on IQR\ndef outlierIQR(comments, score):\n  Outlier = (score>qtlScore[1]) and (comments>qtlComments[1])\n  if Outlier:\n    return True\n  else:\n    return False\noutlierIQR_udf = udf(outlierIQR, BooleanType())\n\n# Computing outliers based on\ndf3 = train_df2.withColumn(\"Outlier\", outlier_udf(col(\"log_num_comments\"), col(\"log_score\")))\ndf3IQR = train_df2.withColumn(\"Outlier\", outlierIQR_udf(col(\"log_num_comments\"), col(\"log_score\")))\ndf4Zscore = df3.where(col(\"Outlier\")==False)# | (col(\"Outlier\")==True)) #can remove outliers if needed\n\n# Printing the change in number of training datapoints due to removal of outliers\nprint(\"Choosing Z-score to detect and eliminate outliers...\")\nprint(\"No. of training datapoints before dropping outliers = {}\".format(df3.agg(F.count(\"test\")).first()[0]))\nprint(\"No. of training datapoints after dropping outliers = {}\".format(df4Zscore.agg(F.count(\"test\")).first()[0]))\n\n# Printing the change in skewness of data as a result of dropping the outliers\nprint(\"Skewness of log_score data before dropping outliers = {}\".format(df3.agg(F.skewness(\"log_score\")).first()[0]))\nprint(\"Skewness of log_score data after dropping outliers = {}\".format(df4Zscore.agg(F.skewness(\"log_score\")).first()[0]))\nprint(\"Skewness of log_num_comments data before dropping outliers = {}\".format(df3.agg(F.skewness(\"log_num_comments\")).first()[0]))\nprint(\"Skewness of log_num_comments data after dropping outliers = {}\".format(df4Zscore.agg(F.skewness(\"log_num_comments\")).first()[0]))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Splitting Training DataFrame into Training and Validation Sets\n# ==============================================================\n\n# Setting the ratio of training set\ntrainSplitRatio = 0.7\n\n# Splitting the majority data into training and validation sets\ntrain_set0, val_set0 = df4Zscore.where(col(\"score\")==0).randomSplit([trainSplitRatio, 1-trainSplitRatio], seed = 420)\n\n# Splitting the remaining (minority) data into training and validation sets\ntrain_set, val_set = df4Zscore.where(col(\"score\")!=0).randomSplit([trainSplitRatio, 1-trainSplitRatio], seed = 125)\n\n# Combining the majority and minority datasets to produce the final training and validation datasets\ntrain_dataset = train_set.union(train_set0).cache()\nval_dataset = val_set.union(val_set0).cache()\n\n# Separating the test data into a new dataframe\ntest_dataset = clean_df.where(col(\"test\")==True)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Dropping Rows with [deleted] Authors from ALL datasets\ndf4 = train_dataset.where(col(\"author\")!=\"[deleted]\")\n\nprint(\"Rows with [deleted] authors removed from test datasets...\\n\")\nprint(\"Training datapoints before dropping [deleted] authors = {}\".format(train_dataset.agg(F.count(\"author\")).first()[0]))\nprint(\"Training datapoints after dropping [deleted] authors = {}\\n\".format(df4.agg(F.count(\"author\")).first()[0]))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["import sparknlp\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.pretrained import PretrainedPipeline\nfrom pyspark.sql.functions import udf"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["document_assembler = DocumentAssembler().setInputCol(\"titleClean\").setOutputCol(\"document\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["document_assembler = DocumentAssembler().setInputCol(\"titleClean\").setOutputCol(\"document\")\ntokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\nword_embeddings = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"document\", \"token\"]).setOutputCol(\"embeddings\")\nbert_pipeline = Pipeline().setStages([document_assembler,tokenizer,word_embeddings])\n\n# Averaging Embeddings across the whole Document\ndef avg_vectors(bert_vectors):\n  print('1')\n  length = len(bert_vectors[0][\"embeddings\"])\n  print('2')\n  avg_vec = [0] * length\n  print('3')\n  for vec in bert_vectors:\n    for index, x in enumerate(vec[\"embeddings\"]):\n      avg_vec[index] =avg_vec[index]+ x\n    avg_vec[index] = avg_vec[index] / length\n  return avg_vec\n\n#create a udf\navg_vectors_udf = F.udf(avg_vectors, T.ArrayType(T.DoubleType()))\n\ndef dense_vector(vec):\n\treturn Vectors.dense(vec)\n  \n# Vectorizing in Spark Vector Assembler Format\ndense_vector_udf = F.udf(dense_vector, VectorUDT())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["pipelineModel = bert_pipeline.fit(train_dataset)\n\n# Train Transformations\ntrain_dataset = pipelineModel.transform(train_dataset)\ntrain_dataset = train_dataset.withColumn(\"doc_vector\", avg_vectors_udf(F.col(\"embeddings\")))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["pipelineModel = bert_pipeline.fit(train_dataset)\n\n# Train Transformations\ntrain_dataset = pipelineModel.transform(train_dataset)\ntrain_dataset = train_dataset.withColumn(\"doc_vector\", avg_vectors_udf(F.col(\"embeddings\")))\ntrain_dataset = train_dataset.withColumn(\"features\", dense_vector_udf(F.col(\"doc_vector\")))\ntrain_set = train_dataset.select(\"features\",\"log_score\")\n\n\n# Val Transformations\nval_set = pipelineModel.transform(val_dataset)\nval_set = val_set.withColumn(\"doc_vector\", avg_vectors_udf(F.col(\"embeddings\")))\nval_set = val_set.withColumn(\"features\", dense_vector_udf(F.col(\"doc_vector\")))\nval_set = val_set.select(\"titleClean\",\"features\",\"log_score\")\n\n\n# Test Transformations\ntest_set = pipelineModel.transform(test_dataset)\ntest_set = test_set.withColumn(\"doc_vector\", avg_vectors_udf(F.col(\"embeddings\")))\ntest_set = test_set.withColumn(\"features\", dense_vector_udf(F.col(\"doc_vector\")))\ntest_set = test_set.select(\"titleClean\",\"features\",\"log_score\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Defining Performance Measurement Functions\n# ==========================================\n\n# Importing libraries\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Root mean square error\ndef rmse(df, predCol=\"predictedScore\", actCol=\"actualScore\"):\n  zero = df.where(col(actCol)==0)\n  nonzero = df.where(col(actCol)!=0)\n  evaluator = RegressionEvaluator(labelCol=actCol, predictionCol=predCol, metricName=\"rmse\")\n  rmse_total = evaluator.evaluate(df)\n  rmse_zero = evaluator.evaluate(zero)\n  rmse_nonzero = evaluator.evaluate(nonzero)\n  return [rmse_total, rmse_zero, rmse_nonzero]\n\n# Symmetric mean absolute percentage error\ndef smape(df, predCol=\"predictedScore\", actCol=\"actualScore\"):\n  zero = df.where(col(actCol)==0)\n  nonzero = df.where(col(actCol)!=0)\n  sm = df.withColumn(\"sm\", 100*((F.abs(col(predCol) - col(actCol)))/(0.5*(F.abs(col(actCol))+F.abs(col(predCol))))))\n  sm_zero = zero.withColumn(\"sm\", 100*((F.abs(col(predCol) - col(actCol)))/(0.5*(F.abs(col(actCol))+F.abs(col(predCol))))))\n  sm_nonzero = nonzero.withColumn(\"sm\", 100*((F.abs(col(predCol) - col(actCol)))/(0.5*(F.abs(col(actCol))+F.abs(col(predCol))))))\n  smape_total = sm.agg(F.mean(\"sm\")).first()[0]\n  smape_zero = sm_zero.agg(F.mean(\"sm\")).first()[0]\n  smape_nonzero = sm_nonzero.agg(F.mean(\"sm\")).first()[0]\n  return [smape_total, smape_zero, smape_nonzero]\n\n# Mean average error\ndef mae(df, predCol=\"predictedScore\", actCol=\"actualScore\"):\n  zero = df.where(col(actCol)==0)\n  nonzero = df.where(col(actCol)!=0)\n  evaluator = RegressionEvaluator(labelCol=actCol, predictionCol=predCol, metricName=\"mae\")\n  mae_total = evaluator.evaluate(df)\n  mae_zero = evaluator.evaluate(zero)\n  mae_nonzero = evaluator.evaluate(nonzero)\n  return [mae_total, mae_zero, mae_nonzero]\n\n# Lightweight regression evaluators\nevaluatorR2 = RegressionEvaluator(labelCol=\"log_score\", predictionCol=\"prediction\", metricName=\"r2\")\nevaluatorMAE = RegressionEvaluator(labelCol=\"actualScore\", predictionCol=\"predictedScore\", metricName=\"mae\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# Training Models for Manual Hyperparameter Search\n# Due to Cluster Limitations [initializing results list]\n# ======================================================\n\nresults_rf = []\ntest_results = []"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Training RF Model for Manual Hyperparameter Search\n# Due to Cluster Limitations [training the model]\n# ==================================================\n\n# Importing libraries\nfrom pyspark.ml.regression import RandomForestRegressor\n\n# Hyperparameters\nnum_trees = 40\nmax_depth = 5\n\nprint(\"Training Random Forest Regressor Model...\")\nrf = RandomForestRegressor(featuresCol=\"features\", \\\n\t\t\t\t\t\t   labelCol=\"log_score\", \\\n\t\t\t\t\t\t   numTrees=num_trees, \\\n\t\t\t\t\t\t   minInstancesPerNode=5, \\\n\t\t\t\t\t\t   maxDepth=max_depth, \\\n\t\t\t\t\t\t   minInfoGain=0.00, \\\n\t\t\t\t\t\t   featureSubsetStrategy=\"auto\") \n\nmodel_rf = rf.fit(train_set)\nprint(\"Model Trained\")\n\n# Transform validation data\npredictions = model_rf.transform(val_set)\noutput = predictions.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput = output.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Validation Data Transformed\")\n\n# Transform training data\npredictions_tr = model_rf.transform(train_set)\noutput_tr = predictions_tr.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_tr = output_tr.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Training Data Transformed\")\n\n# Printing errors\nmae_train = evaluatorMAE.evaluate(output_tr)\nprint(\"Train MAE = {}\".format(mae_train))\nmae_val = evaluatorMAE.evaluate(output)\nprint(\"Validation MAE = {}\".format(mae_val))\n\n# Append results to list\nresults_rf.append((num_trees, max_depth, mae_train, mae_val))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Apply Tuned RF Model to Test Data\n# =================================\n\n# Transform test data\npredictions_tst = model_rf.transform(test_set)\noutput_tst = predictions_tst.withColumn(\"predictedScore\", F.round(F.exp(col(\"prediction\"))-1,0))\noutput_tst = output_tst.withColumn(\"actualScore\", F.round(F.exp(col(\"log_score\"))-1,0))\nprint(\"Test Data Transformed\")\n\n# Printing errors\nmae_test = mae(output_tst)\ntest_results.append((\"Random Forest\", mae_test[0], mae_test[1], mae_test[2]))\nprint(\"Test MAE = {}\".format(mae_test))"],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"step4_RF_BERT","notebookId":3448088735683111},"nbformat":4,"nbformat_minor":0}
